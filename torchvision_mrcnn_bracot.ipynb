{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"torchvision_mrcnn_bracot.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO0mfrgFRUm5hJSk28Styr3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OEqHuQb0yD60"},"source":["# Code for running Torchvision Mask R-CNN on BRACOT dataset \n","*Version for Google Colab*\n","\n","*Based on [this](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)*"]},{"cell_type":"markdown","metadata":{"id":"7Abb6xhuyfrC"},"source":["***Connect to drive (if your data is on GDrive)***"]},{"cell_type":"code","metadata":{"id":"MkjOl9DMyx2b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-hLB3ZpyxYQ"},"source":["***Importing some libs***"]},{"cell_type":"code","metadata":{"id":"SJdsbkQzyVEr"},"source":["import sys\n","import os\n","sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/torchvision_utils') # Insert path to utils/\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","from pycocotools.coco import COCO\n","\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","from engine import train_one_epoch, evaluate\n","from coco_utils import convert_coco_poly_to_mask\n","import utils\n","import transforms as T\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import visualize_maskrcnn_predictions as vis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0IJsSguy64t"},"source":["***Defining the dataset class***"]},{"cell_type":"code","metadata":{"id":"a7KoaGSZzGjG"},"source":["class LeafDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Own coco file\n","        coco = self.coco\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        # path for input image\n","        path = coco.loadImgs(img_id)[0]['file_name']\n","        # open the input image\n","        img = Image.open(os.path.join(self.root, path))\n","        w, h = img.size\n","\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        segmentation = []\n","        boxes = []\n","\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","            segmentation.append(coco_annotation[i]['segmentation'])\n","\n","\n","        masks = convert_coco_poly_to_mask(segmentation, h, w)\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # Labels (In my case, I only one class: target class or background)\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i]['area'])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        # Iscrowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Annotation is in dictionary format\n","        my_annotation = {}\n","        my_annotation[\"masks\"] = masks\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EgSkdOjLzOIO"},"source":["***Defining the model***"]},{"cell_type":"code","metadata":{"id":"FOMQGbdbzQD1"},"source":["def get_transform():\n","    custom_transforms = []\n","    custom_transforms.append(torchvision.transforms.ToTensor())\n","    return torchvision.transforms.Compose(custom_transforms)\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, box_score_thresh=0.7) # box_score_thresh=0.05\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8tT_x0yzWbh"},"source":["***Defining train, eval on dataset and eval on single image functions***"]},{"cell_type":"code","metadata":{"id":"V84yO7O7zgIM"},"source":["'''\n","Function for training on dataset\n","'''\n","def train(path_imgs_train, path_annotation_train, batch_size=1, lr=0.005, epochs=100):\n","    # train on the GPU or on the CPU, if a GPU is not available\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    # our dataset has two classes only - background and leaf\n","    num_classes = 2\n","\n","    dataset = LeafDataset(root=path_imgs_train, \n","                          annotation=path_annotation_train, \n","                          transforms=get_transform())\n","    \n","    # define training and validation data loaders\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, shuffle=False, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","\n","    print(len(data_loader.dataset))\n","    # get the model using our helper function\n","    model = get_model_instance_segmentation(num_classes)\n","\n","    # move model to the right device\n","    model.to(device)\n","\n","    # construct an optimizer\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=lr,\n","                                momentum=0.9, weight_decay=0.0005)\n","    # and a learning rate scheduler\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                   step_size=3,\n","                                                   gamma=0.1)\n","\n","    num_epochs = epochs\n","\n","    for epoch in range(num_epochs):\n","        # train for one epoch, printing every 10 iterations\n","        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","        # update the learning rate\n","        lr_scheduler.step()\n","    torch.save(model.state_dict(), \"model_maskrcnn_final.pth\")\n","\n","\n","'''\n","Function for evaluation on dataset\n","'''\n","def eval_on_dataset(weights_path, path_imgs_test, path_annotation_test, batch_size=1):\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    \n","    model = get_model_instance_segmentation(num_classes=2)\n","    \n","    model.load_state_dict(torch.load(weights_path))\n","\n","    # Define test dataset and dataset loader\n","    dataset_test = LeafDataset(root=path_imgs_test, \n","                               annotation=path_annotation_test, \n","                               transforms=get_transform())\n","    \n","    data_loader_test = torch.utils.data.DataLoader(\n","        dataset_test, batch_size=batch_size, shuffle=False, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","    \n","    model.to(device)\n","    \n","    evaluate(model, data_loader_test, device=device)\n","\n","\n","'''\n","Function for evaluating on single image and create outputs for next framework stage\n","'''\n","# Auxiliary function for creating masks\n","# TODO: implement it in a better way\n","def create_fig (img, mask):\n","    temp = img.copy()\n","    i_, j_, _ = img.shape\n","    for i in range (0, i_):\n","        for j in range (0, j_):\n","            if mask[i][j] < 0.1:\n","                temp[i][j][0] = 1 \n","                temp[i][j][1] = 1 \n","                temp[i][j][2] = 1 \n","    return temp\n","\n","def eval_single_img(weights_path, path_to_img):\n","    # Load model\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    model = get_model_instance_segmentation(num_classes=2)\n","    model.load_state_dict(torch.load(weights_path))\n","    model.eval()\n","    t = get_transform()\n","\n","    # Open image and run through model\n","    img = Image.open(path_to_img)\n","    outputs = model([t(img)])\n","\n","    # Visualize predictions\n","    result, _, _ = vis.predict(img, model.to(device))\n","    cv2_imshow(result)\n","    \n","    # Save cropped masks on files\n","    transpose_fig = np.transpose(t(img).numpy(), (1,2,0))\n","    masks = outputs[0]['masks'].ge(0.5).mul(255).byte().cpu().numpy()\n","    \n","    if not os.path.exists('/content/outputs/'):\n","        os.makedirs('/content/outputs/instance_seg_result')\n","    cv2.imwrite(f'/content/outputs/instance_seg_result/instanceseg.png', result)\n","    \n","    i = 0\n","    for mask in masks:\n","        temp = create_fig(transpose_fig, mask[0])\n","        toTensor = torchvision.transforms.ToTensor()\n","        toPIL = torchvision.transforms.ToPILImage()\n","        temp = toPIL(toTensor(temp))\n","        temp.save(f'/content/outputs/instance_seg_result/{i}.jpg')\n","        i += 1\n","        plt.clf()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lfnl479P3E13"},"source":["### Example"]},{"cell_type":"markdown","metadata":{"id":"tRWIM3o73NFb"},"source":["***Defining some params***"]},{"cell_type":"code","metadata":{"id":"-bG6x6zC3K8S"},"source":["# path to root folder containing train imgs and annotations\n","path_imgs_train = '/content/drive/My Drive/ic2-dataset/train/'\n","path_annotation_train = '/content/drive/My Drive/ic2-dataset/train/train_annotation.json'\n","\n","# path to root folder containing test imgs and annotations\n","path_imgs_test =  '/content/drive/My Drive/ic2-dataset/test/'\n","path_annotation_test = '/content/drive/My Drive/ic2-dataset/test/test_annotation.json'\n","\n","# batch size\n","batch_size = 1\n","# learning rate\n","lr = 0.005\n","# num of epochs\n","epochs = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Frd9XwDh4iqx"},"source":["***Training on dataset***"]},{"cell_type":"code","metadata":{"id":"-_5Sk-Jq4h2B"},"source":["# train\n","train(path_imgs_train=path_imgs_train, \n","      path_annotation_train=path_annotation_train, \n","      batch_size=batch_size, \n","      lr=lr, \n","      epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZjiarWp40ji"},"source":["***Evaluating on dataset***"]},{"cell_type":"code","metadata":{"id":"crAgyT0c45ol"},"source":["# path to model weights\n","weights_path = '/content/drive/My Drive/ic2-dataset/model_final_torchvision_100epochs.pth'\n","\n","# eval\n","eval_on_dataset(weights_path=weights_path, \n","                path_imgs_test=path_imgs_test, \n","                path_annotation_test=path_annotation_test, \n","                batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DR-dejNz5h8P"},"source":["***Evaluating on single image***"]},{"cell_type":"code","metadata":{"id":"0yIqZrm85k_x"},"source":["# define img path and weights path\n","weights_path = '/content/drive/My Drive/ic2-dataset/model_final_torchvision_100epochs.pth'\n","img_path = '/content/drive/MyDrive/ic2-dataset/test/20190831_163222.jpg'\n","\n","# eval\n","eval_single_img(weights_path = weights_path, \n","                path_to_img = img_path)"],"execution_count":null,"outputs":[]}]}